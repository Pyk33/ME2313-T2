{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/_0j_111n661ccjxlkkzpn6xh0000gn/T/ipykernel_95391/2513161849.py:22: DtypeWarning: Columns (9,14,19,22,24,29,32,34,39,40,41,42,43,44,45,48,52,75,87,91,99,110,112,113,135,139,140,141,143,145,156,157) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(input_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to /Users/ollepyk/Documents/GitHub/ME2313-T2/Cleaned Data/RT.IRS_Clean_v1.csv\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 1: Imports RT.IRS_data.csv and cleans it by removing all rows which does  not have \n",
    "# data in the \"Event\"-column. Further it removes all headers and corresponding \n",
    "# columns which is empty (no data).\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths using relative paths\n",
    "input_file_path = \"Categorized Data/RT.IRS_Data.csv\"\n",
    "output_file_path = \"Cleaned Data/RT.IRS_Clean_v1.csv\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct absolute file paths\n",
    "input_file_path = os.path.join(current_directory, input_file_path)\n",
    "output_file_path = os.path.join(current_directory, output_file_path)\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(input_file_path)\n",
    "\n",
    "# Remove rows where the \"Event\" column is empty\n",
    "data = data.dropna(subset=['Event'])\n",
    "\n",
    "# Remove columns where all values are NaN after removing empty rows\n",
    "data = data.dropna(axis=1, how='all')\n",
    "\n",
    "# Remove rows where \"Event\" column starts with two numbers\n",
    "data = data[~data['Event'].astype(str).str.match(r'^\\d{2}')]\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Data cleaned and saved to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned, sorted, and saved to /Users/ollepyk/Documents/GitHub/ME2313-T2/Cleaned Data/RT.IRS_Clean_v1.csv\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 2: Sort the rows based on the \"Event\" column \n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Sort the rows based on the \"Event\" column\n",
    "data = data.sort_values(by='Event')\n",
    "\n",
    "# Save the cleaned and sorted data to a new CSV file\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Data cleaned, sorted, and saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Event', 'Execution Timestamp', 'Dissemination Time', 'Cleared',\n",
       "        'Collateralization', 'End-User Exception', 'Bespoke',\n",
       "        'Block/Off facility', 'Execution Venue', 'UPI', 'Product',\n",
       "        'Contract Type', 'Effective Date', 'Maturity Date', 'Upfront Payment',\n",
       "        'Upfront Payment Date', 'Settlement Currency', 'Leg 1 Type',\n",
       "        'Leg 1 Fixed Rate', 'Leg 1 Floating Index', 'Leg 1 Designated Maturity',\n",
       "        'Leg 1 Spread', 'Leg 1 Day Count Convention', 'Leg 1 Notional',\n",
       "        'Leg 1 Notional Currency', 'Leg 1 Payment Frequency',\n",
       "        'Leg1 Reset Frequency', 'Leg 2 Type', 'Leg 2 Fixed Rate',\n",
       "        'Leg 2 Floating Index', 'Leg 2 Designated Maturity', 'Leg 2 Spread',\n",
       "        'Leg 2 Day Count Convention', 'Leg 2 Notional',\n",
       "        'Leg 2 Notional Currency', 'Leg 2 Payment Frequency',\n",
       "        'Leg 2 Reset Frequency', 'Embedded Option', 'Option Strike Price',\n",
       "        'Option Family', 'Option Premium', 'Option Expiration Date', 'Rpt ID',\n",
       "        'Prev Rpt ID', 'Contract Subtype'],\n",
       "       dtype='object'),\n",
       " (27326, 45))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 3: Remove columns with only one unique value\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Identify and drop columns with only one unique value\n",
    "cols_to_drop = [col for col in data.columns if data[col].nunique() == 1]\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the remaining columns and the shape of the dataset\n",
    "remaining_columns = data.columns\n",
    "data_shape = data.shape\n",
    "\n",
    "remaining_columns, data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage for each column (columns with coverage less than 20% removed):\n",
      "                        Column    Coverage\n",
      "0                        Event  100.000000\n",
      "13               Maturity Date  100.000000\n",
      "42                      Rpt ID  100.000000\n",
      "40              Option Premium  100.000000\n",
      "38         Option Strike Price  100.000000\n",
      "30   Leg 2 Designated Maturity  100.000000\n",
      "24     Leg 1 Notional Currency  100.000000\n",
      "23              Leg 1 Notional  100.000000\n",
      "1          Execution Timestamp  100.000000\n",
      "20   Leg 1 Designated Maturity  100.000000\n",
      "16         Settlement Currency  100.000000\n",
      "14             Upfront Payment  100.000000\n",
      "22  Leg 1 Day Count Convention  100.000000\n",
      "12              Effective Date  100.000000\n",
      "3                      Cleared  100.000000\n",
      "11               Contract Type  100.000000\n",
      "7           Block/Off facility  100.000000\n",
      "6                      Bespoke  100.000000\n",
      "2           Dissemination Time  100.000000\n",
      "10                     Product  100.000000\n",
      "28            Leg 2 Fixed Rate   99.978043\n",
      "18            Leg 1 Fixed Rate   99.974383\n",
      "21                Leg 1 Spread   99.952426\n",
      "31                Leg 2 Spread   99.952426\n",
      "33              Leg 2 Notional   99.923150\n",
      "8              Execution Venue   97.712801\n",
      "25     Leg 1 Payment Frequency   97.123619\n",
      "17                  Leg 1 Type   97.123619\n",
      "32  Leg 2 Day Count Convention   96.995535\n",
      "34     Leg 2 Notional Currency   96.995535\n",
      "27                  Leg 2 Type   96.995535\n",
      "35     Leg 2 Payment Frequency   96.995535\n",
      "4            Collateralization   96.779624\n",
      "36       Leg 2 Reset Frequency   92.519944\n",
      "26        Leg1 Reset Frequency   92.494328\n",
      "29        Leg 2 Floating Index   92.487009\n",
      "37             Embedded Option   19.186855\n",
      "19        Leg 1 Floating Index    7.578863\n",
      "44            Contract Subtype    4.391422\n",
      "15        Upfront Payment Date    3.890068\n",
      "5           End-User Exception    1.010027\n",
      "9                          UPI    0.285442\n",
      "43                 Prev Rpt ID    0.285442\n",
      "39               Option Family    0.179316\n",
      "41      Option Expiration Date    0.179316\n",
      "\n",
      "Removed columns:\n",
      "           Removed Column   Coverage\n",
      "1         Embedded Option  19.186855\n",
      "2    Leg 1 Floating Index   7.578863\n",
      "3        Contract Subtype   4.391422\n",
      "4    Upfront Payment Date   3.890068\n",
      "5      End-User Exception   1.010027\n",
      "6                     UPI   0.285442\n",
      "7             Prev Rpt ID   0.285442\n",
      "8           Option Family   0.179316\n",
      "9  Option Expiration Date   0.179316\n",
      "Filtered data saved to Cleaned Data/RT.IRS_Clean_v2.csv\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 4: Check the coverage for all the columns in the CSV file and remove columns with coverage less than 20%\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Calculate the percentage of non-null values in each column\n",
    "coverage = (data.count() / len(data)) * 100\n",
    "\n",
    "# Create a DataFrame to store the coverage information\n",
    "coverage_df = pd.DataFrame({'Column': coverage.index, 'Coverage': coverage.values})\n",
    "\n",
    "# Sort the coverage DataFrame by coverage percentage in descending order\n",
    "coverage_df = coverage_df.sort_values(by='Coverage', ascending=False)\n",
    "\n",
    "# Remove columns with coverage less than 20%\n",
    "columns_to_keep = coverage_df[coverage_df['Coverage'] >= 20]['Column']\n",
    "removed_columns_df = coverage_df[coverage_df['Coverage'] < 20]\n",
    "\n",
    "# Store removed columns information in a separate DataFrame\n",
    "removed_columns_df = removed_columns_df.rename(columns={'Column': 'Removed Column', 'Coverage': 'Coverage'})\n",
    "removed_columns_df.index = range(1, len(removed_columns_df) + 1)\n",
    "\n",
    "filtered_data = data[columns_to_keep]\n",
    "\n",
    "# Save the cleaned and filtered data to a new CSV file in the Cleaned Data folder\n",
    "filtered_output_file_path = \"Cleaned Data/RT.IRS_Clean_v2.csv\"\n",
    "filtered_data.to_csv(filtered_output_file_path, index=False)\n",
    "\n",
    "# Print the coverage information\n",
    "print(\"Coverage for each column (columns with coverage less than 20% removed):\")\n",
    "print(coverage_df)\n",
    "\n",
    "# Print the list of removed columns in a formatted way\n",
    "if not removed_columns_df.empty:\n",
    "    print(\"\\nRemoved columns:\")\n",
    "    print(removed_columns_df)\n",
    "else:\n",
    "    print(\"\\nNo columns were removed.\")\n",
    "\n",
    "print(\"Filtered data saved to\", filtered_output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Leg 2 Floating Index          7.512991\n",
       "Leg1 Reset Frequency          7.505672\n",
       "Leg 2 Reset Frequency         7.480056\n",
       "Collateralization             3.220376\n",
       "Leg 2 Payment Frequency       3.004465\n",
       "Leg 2 Type                    3.004465\n",
       "Leg 2 Notional Currency       3.004465\n",
       "Leg 2 Day Count Convention    3.004465\n",
       "Leg 1 Payment Frequency       2.876381\n",
       "Leg 1 Type                    2.876381\n",
       "Execution Venue               2.287199\n",
       "Leg 2 Notional                0.076850\n",
       "Leg 2 Spread                  0.047574\n",
       "Leg 1 Spread                  0.047574\n",
       "Leg 1 Fixed Rate              0.025617\n",
       "Leg 2 Fixed Rate              0.021957\n",
       "Maturity Date                 0.000000\n",
       "Product                       0.000000\n",
       "Event                         0.000000\n",
       "Bespoke                       0.000000\n",
       "Execution Timestamp           0.000000\n",
       "Rpt ID                        0.000000\n",
       "Option Premium                0.000000\n",
       "Option Strike Price           0.000000\n",
       "Leg 2 Designated Maturity     0.000000\n",
       "Leg 1 Notional Currency       0.000000\n",
       "Leg 1 Notional                0.000000\n",
       "Leg 1 Designated Maturity     0.000000\n",
       "Block/Off facility            0.000000\n",
       "Settlement Currency           0.000000\n",
       "Upfront Payment               0.000000\n",
       "Leg 1 Day Count Convention    0.000000\n",
       "Effective Date                0.000000\n",
       "Cleared                       0.000000\n",
       "Contract Type                 0.000000\n",
       "Dissemination Time            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 5: Check missing coverage of remaining columns after cleanse\n",
    "#--------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Cleaned Data/RT.IRS_Clean_v2.csv\")\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = data.isnull().sum() / len(data) * 100\n",
    "\n",
    "# Display the columns with their missing values percentage\n",
    "missing_percentage.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing:  0\n",
      "Numerical columns:  Index(['Option Premium', 'Option Strike Price', 'Leg 1 Notional',\n",
      "       'Upfront Payment', 'Leg 2 Fixed Rate', 'Leg 1 Fixed Rate',\n",
      "       'Leg 1 Spread', 'Leg 2 Spread', 'Leg 2 Notional'],\n",
      "      dtype='object')\n",
      "Categorical columns:  Index(['Event', 'Maturity Date', 'Rpt ID', 'Leg 2 Designated Maturity',\n",
      "       'Leg 1 Notional Currency', 'Execution Timestamp',\n",
      "       'Leg 1 Designated Maturity', 'Settlement Currency',\n",
      "       'Leg 1 Day Count Convention', 'Effective Date', 'Cleared',\n",
      "       'Contract Type', 'Block/Off facility', 'Bespoke', 'Dissemination Time',\n",
      "       'Product', 'Execution Venue', 'Leg 1 Payment Frequency', 'Leg 1 Type',\n",
      "       'Leg 2 Day Count Convention', 'Leg 2 Notional Currency', 'Leg 2 Type',\n",
      "       'Leg 2 Payment Frequency', 'Collateralization', 'Leg 2 Reset Frequency',\n",
      "       'Leg1 Reset Frequency', 'Leg 2 Floating Index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 6: Fill missing values\n",
    "#   1. Fill numerical columns with their median (since median is less sensitive to outliers than mean).\n",
    "#   2. Fill categorical columns with their mode (most frequent value).\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill missing values in numerical columns with their median\n",
    "for column in numerical_columns:\n",
    "    median_value = data[column].median()\n",
    "    data[column].fillna(median_value, inplace=True)\n",
    "\n",
    "# Fill missing values in categorical columns with their mode\n",
    "for column in categorical_columns:\n",
    "    mode_value = data[column].mode()[0]\n",
    "    data[column].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "remaining_missing = data.isnull().sum().sum()\n",
    "\n",
    "print(\"Remaining missing: \", remaining_missing)\n",
    "print(\"Numerical columns: \", numerical_columns)\n",
    "print(\"Categorical columns: \", categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: 27 unique values\n",
      "Maturity Date: 4017 unique values\n",
      "Rpt ID: 27323 unique values\n",
      "Leg 2 Designated Maturity: 451 unique values\n",
      "Leg 1 Notional Currency: 27 unique values\n",
      "Execution Timestamp: 2124 unique values\n",
      "Leg 1 Designated Maturity: 453 unique values\n",
      "Settlement Currency: 27 unique values\n",
      "Leg 1 Day Count Convention: 7 unique values\n",
      "Effective Date: 2458 unique values\n",
      "Cleared: 4 unique values\n",
      "Contract Type: 4 unique values\n",
      "Block/Off facility: 2 unique values\n",
      "Bespoke: 2 unique values\n",
      "Dissemination Time: 2125 unique values\n",
      "Product: 885 unique values\n",
      "Execution Venue: 3 unique values\n",
      "Leg 1 Payment Frequency: 9 unique values\n",
      "Leg 1 Type: 4 unique values\n",
      "Leg 2 Day Count Convention: 6 unique values\n",
      "Leg 2 Notional Currency: 27 unique values\n",
      "Leg 2 Type: 4 unique values\n",
      "Leg 2 Payment Frequency: 9 unique values\n",
      "Collateralization: 3 unique values\n",
      "Leg 2 Reset Frequency: 7 unique values\n",
      "Leg1 Reset Frequency: 7 unique values\n",
      "Leg 2 Floating Index: 229 unique values\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 7: List unique values count for each categorical column \n",
    "#   1. Used to understand how extensive the one hot encoding will be\n",
    "#   2. Reveals if any columns shuld be removed (too many unique values)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "unique_counts = {}\n",
    "\n",
    "for column in categorical_columns:\n",
    "    unique_counts[column] = data[column].nunique()\n",
    "\n",
    "# Display the counts\n",
    "for column, count in unique_counts.items():\n",
    "    print(f\"{column}: {count} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns: ['Rpt ID']\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 8: Manually remove selected columns\n",
    "#   1. Specify the columns to be removed in a list.\n",
    "#   2. Use the drop method to remove these columns from the dataframe.\n",
    "#   3. Store the removed columns in a separate list for future reference.\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# List of columns to be removed\n",
    "columns_to_remove = ['Rpt ID']  # , 'Maturity Date', 'Execution Timestamp' You can update this list with actual column names you want to remove\n",
    "\n",
    "# Remove the columns and store them in a separate list\n",
    "removed_columns = []\n",
    "for column in columns_to_remove:\n",
    "    if column in data.columns:\n",
    "        data.drop(column, axis=1, inplace=True)\n",
    "        removed_columns.append(column)\n",
    "\n",
    "# Display the list of removed columns\n",
    "print(\"Removed columns:\", removed_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 9: Date/timestamp management\n",
    "#   1. Convert \"Maturity Date\" and \"Execution Timestamp\" columns to datetime format.\n",
    "#   2. Extract relevant features from these datetime columns.\n",
    "#   3. Convert derived date features to categorical values.\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "# Convert \"Maturity Date\" and \"Execution Timestamp\" columns to datetime format\n",
    "#data[\"Maturity Date\"] = pd.to_datetime(data[\"Maturity Date\"], errors='coerce')\n",
    "#data[\"Execution Timestamp\"] = pd.to_datetime(data[\"Execution Timestamp\"], errors='coerce')\n",
    "\n",
    "# Extract features from \"Maturity Date\"\n",
    "#data[\"Maturity_Day\"] = data[\"Maturity Date\"].dt.day\n",
    "#data[\"Maturity_Month\"] = data[\"Maturity Date\"].dt.month\n",
    "#data[\"Maturity_Year\"] = data[\"Maturity Date\"].dt.year\n",
    "\n",
    "# Extract features from \"Execution Timestamp\"\n",
    "#data[\"Execution_Day\"] = data[\"Execution Timestamp\"].dt.day\n",
    "#data[\"Execution_Month\"] = data[\"Execution Timestamp\"].dt.month\n",
    "#data[\"Execution_Year\"] = data[\"Execution Timestamp\"].dt.year\n",
    "\n",
    "# Convert the derived features to strings, making them categorical\n",
    "#categorical_columns = [\"Maturity_Day\", \"Maturity_Month\", \"Maturity_Year\", \n",
    "#                       \"Execution_Day\", \"Execution_Month\", \"Execution_Year\"]\n",
    "\n",
    "#for col in categorical_columns:\n",
    "#    data[col] = data[col].astype(str)\n",
    "\n",
    "# Drop the original datetime columns\n",
    "#data.drop([\"Maturity Date\", \"Execution Timestamp\"], axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows with the new features\n",
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27326, 12904),\n",
       "    Option Premium  Option Strike Price  Leg 1 Notional  Upfront Payment  \\\n",
       " 0             0.0                  0.0    9.979034e-07         0.496424   \n",
       " 1             0.0                  0.0    9.979034e-07         0.496424   \n",
       " 2             0.0                  0.0    9.979034e-07         0.496424   \n",
       " 3             0.0                  0.0    9.979034e-07         0.496424   \n",
       " 4             1.0                  0.0    1.007988e-05         0.496424   \n",
       " \n",
       "    Leg 2 Fixed Rate  Leg 1 Fixed Rate  Leg 1 Spread  Leg 2 Spread  \\\n",
       " 0          0.013324          0.002164      0.000000           0.0   \n",
       " 1          0.003358          0.008587      0.000000           0.0   \n",
       " 2          0.003358          0.008587      0.000000           0.0   \n",
       " 3          0.013324          0.002164      0.000000           0.0   \n",
       " 4          0.003358          0.644479      0.753012           0.0   \n",
       " \n",
       "    Leg 2 Notional  Event_Amendment  ...  \\\n",
       " 0    9.979084e-07            False  ...   \n",
       " 1    9.979084e-07            False  ...   \n",
       " 2    9.979084e-07            False  ...   \n",
       " 3    9.979084e-07            False  ...   \n",
       " 4    0.000000e+00             True  ...   \n",
       " \n",
       "    Leg 2 Floating Index_USD.USD-LIBOR-BBA.3.  \\\n",
       " 0                                      False   \n",
       " 1                                      False   \n",
       " 2                                      False   \n",
       " 3                                      False   \n",
       " 4                                      False   \n",
       " \n",
       "    Leg 2 Floating Index_USD.USD-LIBOR-BBA.3M.USD-LIBOR-BBA  \\\n",
       " 0                                               True         \n",
       " 1                                              False         \n",
       " 2                                              False         \n",
       " 3                                               True         \n",
       " 4                                               True         \n",
       " \n",
       "    Leg 2 Floating Index_USD.USD-LIBOR-BBA.6M.USD-LIBOR-BBA  \\\n",
       " 0                                              False         \n",
       " 1                                              False         \n",
       " 2                                              False         \n",
       " 3                                              False         \n",
       " 4                                              False         \n",
       " \n",
       "    Leg 2 Floating Index_USD.USD-SOFR-COMPOUND..  \\\n",
       " 0                                         False   \n",
       " 1                                         False   \n",
       " 2                                         False   \n",
       " 3                                         False   \n",
       " 4                                         False   \n",
       " \n",
       "    Leg 2 Floating Index_USD.USD-SOFR-COMPOUND..USD-SOFR-COMPOUND  \\\n",
       " 0                                              False               \n",
       " 1                                              False               \n",
       " 2                                              False               \n",
       " 3                                              False               \n",
       " 4                                              False               \n",
       " \n",
       "    Leg 2 Floating Index_USD.USD-SOFR-COMPOUND.1Y.USD-SOFR-COMPOUND  \\\n",
       " 0                                              False                 \n",
       " 1                                              False                 \n",
       " 2                                              False                 \n",
       " 3                                              False                 \n",
       " 4                                              False                 \n",
       " \n",
       "    Leg 2 Floating Index_ZAR-JIBAR  Leg 2 Floating Index_ZAR-JIBAR-SAFEX  \\\n",
       " 0                           False                                 False   \n",
       " 1                           False                                 False   \n",
       " 2                           False                                 False   \n",
       " 3                           False                                 False   \n",
       " 4                           False                                 False   \n",
       " \n",
       "    Leg 2 Floating Index_ZAR.JIBAR.3M.SAFEX  \\\n",
       " 0                                    False   \n",
       " 1                                    False   \n",
       " 2                                    False   \n",
       " 3                                    False   \n",
       " 4                                    False   \n",
       " \n",
       "    Leg 2 Floating Index_ZAR.ZAR-JIBAR-SAFEX.3M.ZAR-JIBAR-SAFEX  \n",
       " 0                                              False            \n",
       " 1                                              False            \n",
       " 2                                              False            \n",
       " 3                                              False            \n",
       " 4                                              False            \n",
       " \n",
       " [5 rows x 12904 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 10: Normalization for machine learning suitability\n",
    "#   1. Normalize numerical features to ensure they have a similar scale.\n",
    "#   2. One-hot encode categorical features to convert them into a format suitable for the machine learning model.\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "data_encoded = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Display the shape and first few rows of the transformed dataset\n",
    "data_encoded_shape = data_encoded.shape\n",
    "data_encoded_head = data_encoded.head()\n",
    "\n",
    "data_encoded_shape, data_encoded_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Module 11: Pickle that big boi data for later use in GAN model\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ensure the directory \"Processed data\" exists\n",
    "if not os.path.exists(\"Processed data\"):\n",
    "    os.makedirs(\"Processed data\")\n",
    "\n",
    "# Save the DataFrame as a pickled file\n",
    "with open(\"Processed data/data_encoded.pkl\", \"wb\") as file:\n",
    "    pickle.dump(data_encoded, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
